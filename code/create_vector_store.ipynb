{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Literal\n",
    "from PIL import Image\n",
    "from collections import Counter, defaultdict\n",
    "import re, math, json\n",
    "import fitz           \n",
    "import pdfplumber\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "import hashlib\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import cohere\n",
    "from copy import deepcopy\n",
    "\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "from io import BytesIO\n",
    "import tqdm\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.retrievers.multi_vector import SearchType\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from uuid import uuid4\n",
    "import time\n",
    "import gzip, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = '../instructions/Dataset/'\n",
    "image_output_path = '../generated_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение на чанки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_RE = re.compile(r\"^[\\s\\-\\+\\(\\)]*[\\d]+([.,\\s]\\d+)*\\s*[%₸$€₽KZTUSD EURRUB]*$\", re.I)\n",
    "\n",
    "def is_numericish(s: str) -> bool:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s: return False\n",
    "    if re.search(r\"\\d\", s) and len(s) <= 32:\n",
    "        return True if NUM_RE.search(s) else False\n",
    "    return False\n",
    "\n",
    "def frac_numeric_cells(table_rows: List[List[str]]) -> float:\n",
    "    cells = [c for row in table_rows for c in row]\n",
    "    if not cells: return 0.0\n",
    "    return sum(is_numericish(c) for c in cells) / len(cells)\n",
    "\n",
    "def frac_numeric_rows(table_rows: List[List[str]]) -> float:\n",
    "    if not table_rows: return 0.0\n",
    "    cnt = 0\n",
    "    for row in table_rows:\n",
    "        if not row: \n",
    "            continue\n",
    "        nonempty = [c for c in row if (c or \"\").strip()]\n",
    "        if not nonempty:\n",
    "            continue\n",
    "        if sum(is_numericish(c) for c in nonempty) / len(nonempty) >= 0.5:\n",
    "            cnt += 1\n",
    "    return cnt / max(1, len(table_rows))\n",
    "\n",
    "def table_quality_ok(rows: List[List[str]]) -> bool:\n",
    "    if not rows: return False\n",
    "    R = len(rows)\n",
    "    C = max((len(r) for r in rows), default=0)\n",
    "    if R < 2 or C < 2:\n",
    "        return False\n",
    "    cells = [c for r in rows for c in r]\n",
    "    nonempty_frac = sum(1 for c in cells if (c or \"\").strip()) / max(1, len(cells))\n",
    "    if nonempty_frac < 0.35:\n",
    "        return False\n",
    "    if frac_numeric_rows(rows) < 0.25:  \n",
    "        return False\n",
    "    mean_len = sum(len((c or \"\").strip()) for c in cells) / max(1, len(cells))\n",
    "    if mean_len > 120:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def list_to_camelot_pages(pages_1based: List[int]) -> str:\n",
    "    if not pages_1based: return \"\"\n",
    "    pages = sorted(set(pages_1based))\n",
    "    rngs, s = [], pages[0]\n",
    "    for p in pages[1:]:\n",
    "        if p != pages[pages.index(p)-1] + 1:\n",
    "            rngs.append((s, pages[pages.index(p)-1]))\n",
    "            s = p\n",
    "    rngs.append((s, pages[-1]))\n",
    "    return \",\".join(f\"{a}-{b}\" if a != b else f\"{a}\" for a,b in rngs)\n",
    "\n",
    "# ===================== 1) скрининг страниц =====================\n",
    "\n",
    "def screen_pages(pdf_path: str, min_words: int = 10) -> Tuple[List[int], List[int]]:\n",
    "    text_pages, image_pages = [], []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    try:\n",
    "        for i in range(len(doc)):\n",
    "            words = doc[i].get_text(\"words\")\n",
    "            (text_pages if len(words) >= min_words else image_pages).append(i+1)\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return text_pages, image_pages\n",
    "\n",
    "# ===================== 2) оценка «табличности» страницы =====================\n",
    "\n",
    "def rulings_score(pdf_path: str, page_no: int, min_len: float = 60.0) -> float:\n",
    "    \"\"\"\n",
    "    Считает количество длинных горизонтальных/вертикальных линий на странице\n",
    "    по векторной графике (drawings). Возвращает скаляр 0..1.\n",
    "    Поддерживает tuple-формат items из get_drawings().\n",
    "    \"\"\"\n",
    "    def as_xy(pt):\n",
    "        # pt может быть fitz.Point или (x, y)\n",
    "        try:\n",
    "            return float(pt[0]), float(pt[1])\n",
    "        except Exception:\n",
    "            return float(pt.x), float(pt.y)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    try:\n",
    "        page = doc[page_no - 1]\n",
    "        drawings = page.get_drawings()\n",
    "        H = V = 0\n",
    "\n",
    "        for d in drawings:\n",
    "            items = d[\"items\"] if isinstance(d, dict) else getattr(d, \"items\", [])\n",
    "            for it in items:\n",
    "                if isinstance(it, tuple):\n",
    "                    op = it[0]\n",
    "\n",
    "                    if op == \"l\" and len(it) >= 3:\n",
    "                        p0, p1 = it[1], it[2]\n",
    "                        x0, y0 = as_xy(p0)\n",
    "                        x1, y1 = as_xy(p1)\n",
    "                        dx, dy = abs(x1 - x0), abs(y1 - y0)\n",
    "                        length = math.hypot(dx, dy)\n",
    "                        if length >= min_len:\n",
    "                            if dy <= 1.0: H += 1\n",
    "                            if dx <= 1.0: V += 1\n",
    "\n",
    "                    elif op == \"re\" and len(it) >= 2:\n",
    "                        rect = it[1]\n",
    "                        try:\n",
    "                            x0, y0, x1, y1 = float(rect.x0), float(rect.y0), float(rect.x1), float(rect.y1)\n",
    "                        except Exception:\n",
    "                            x0, y0, x1, y1 = rect  # если это tuple\n",
    "                        edges = [\n",
    "                            (x0, y0, x1, y0), (x1, y0, x1, y1),\n",
    "                            (x1, y1, x0, y1), (x0, y1, x0, y0),\n",
    "                        ]\n",
    "                        for ex0, ey0, ex1, ey1 in edges:\n",
    "                            dx, dy = abs(ex1 - ex0), abs(ey1 - ey0)\n",
    "                            length = math.hypot(dx, dy)\n",
    "                            if length >= min_len:\n",
    "                                if dy <= 1.0: H += 1\n",
    "                                if dx <= 1.0: V += 1\n",
    "\n",
    "\n",
    "                elif isinstance(it, dict):\n",
    "                    pts = it.get(\"points\")\n",
    "                    if pts and len(pts) >= 2:\n",
    "                        p0, p1 = pts[0], pts[-1]\n",
    "                        x0, y0 = as_xy(p0)\n",
    "                        x1, y1 = as_xy(p1)\n",
    "                        dx, dy = abs(x1 - x0), abs(y1 - y0)\n",
    "                        length = math.hypot(dx, dy)\n",
    "                        if length >= min_len:\n",
    "                            if dy <= 1.0: H += 1\n",
    "                            if dx <= 1.0: V += 1\n",
    "\n",
    "        return min(1.0, (H / 6.0 + V / 4.0))\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def column_grid_score(pdf_path: str, page_no: int, x_tol: int = 6) -> float:\n",
    "    \"\"\"\n",
    "    На основе слов: считаем повторяющиеся x-координаты начала слов (кластеры колонок).\n",
    "    Чем больше устойчивых вертикалей и строк, тем «табличнее».\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    try:\n",
    "        page = doc[page_no-1]\n",
    "        words = page.get_text(\"words\") \n",
    "        if not words:\n",
    "            return 0.0\n",
    "        rows = {}\n",
    "        for x0,y0,x1,y1,txt, *_ in words:\n",
    "            key = round(y0/5)*5\n",
    "            rows.setdefault(key, []).append((x0, txt))\n",
    "        xbins = Counter()\n",
    "        row_cnt = 0\n",
    "        for _, items in rows.items():\n",
    "            if len(items) < 3:\n",
    "                continue\n",
    "            row_cnt += 1\n",
    "            xs = [round(x/x_tol)*x_tol for x,_ in items]\n",
    "            for x in set(xs):\n",
    "                xbins[x] += 1\n",
    "        if row_cnt == 0:\n",
    "            return 0.0\n",
    "        stable_cols = sum(1 for _,c in xbins.items() if c >= max(2, int(0.3*row_cnt)))\n",
    "        # нормировка: 0..1\n",
    "        return min(1.0, stable_cols / 6.0)\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def pick_table_pages(pdf_path: str, text_pages: List[int],\n",
    "                     thr_rulings: float = 0.25, thr_cols: float = 0.35) -> List[int]:\n",
    "    candidates = []\n",
    "    for p in text_pages:\n",
    "        rs = rulings_score(pdf_path, p)\n",
    "        cs = column_grid_score(pdf_path, p)\n",
    "        if (rs >= thr_rulings and cs >= thr_cols) or (rs >= thr_rulings*2) or (cs >= thr_cols*1.5):\n",
    "            candidates.append(p)\n",
    "    return candidates\n",
    "\n",
    "# ===================== 3) извлечение таблиц только на кандидатов =====================\n",
    "\n",
    "def extract_tables_on_candidates(pdf_path: str, candidate_pages: List[int]) -> List[Dict[str, Any]]:\n",
    "    tables: List[Dict[str, Any]] = []\n",
    "    if not candidate_pages:\n",
    "        return tables\n",
    "\n",
    "    pages_str = list_to_camelot_pages(candidate_pages)\n",
    "\n",
    "    def _add_rows(rows: List[List[str]], page: int, parser: str):\n",
    "        if not table_quality_ok(rows):\n",
    "            return\n",
    "        csv = \"\\n\".join([\",\".join([(c or \"\").replace(\",\", \" \") for c in r]) for r in rows])\n",
    "        flat = \"\\n\".join([\",\".join([(c or \"\") for c in r]) for r in rows])\n",
    "        tables.append({\n",
    "            \"type\": \"table\",\n",
    "            \"parser\": parser,\n",
    "            \"page\": page,\n",
    "            \"text\": flat,\n",
    "            \"table_csv\": csv,\n",
    "            \"source_pdf\": pdf_path,\n",
    "        })\n",
    "\n",
    "    tried_camelot = False\n",
    "    try:\n",
    "        import camelot\n",
    "        tried_camelot = True\n",
    "        try:\n",
    "            tabs = camelot.read_pdf(pdf_path, pages=pages_str, flavor=\"lattice\",\n",
    "                                    line_scale=40, process_background=True, strip_text=\"\\n\")\n",
    "            for t in tabs:\n",
    "                page = t.parsing_report.get(\"page\")\n",
    "                rows = t.df.values.tolist()\n",
    "                _add_rows(rows, page, \"camelot-lattice\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            tabs = camelot.read_pdf(pdf_path, pages=pages_str, flavor=\"stream\",\n",
    "                                    edge_tol=150, row_tol=10, column_tol=20, strip_text=\"\\n\")\n",
    "            for t in tabs:\n",
    "                page = t.parsing_report.get(\"page\")\n",
    "                rows = t.df.values.tolist()\n",
    "                _add_rows(rows, page, \"camelot-stream\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for p in candidate_pages:\n",
    "            page = pdf.pages[p-1]\n",
    "            settings = dict(\n",
    "                vertical_strategy=\"lines\",\n",
    "                horizontal_strategy=\"lines\",\n",
    "                snap_tolerance=3,\n",
    "                join_tolerance=3,\n",
    "                edge_min_length=20,\n",
    "                intersection_x_tolerance=5,\n",
    "                intersection_y_tolerance=5,\n",
    "            )\n",
    "            found = page.extract_tables(settings) or []\n",
    "            if not found:\n",
    "                found = page.extract_tables() or []\n",
    "            for tbl in found:\n",
    "                rows = [[(c or \"\").strip() for c in row] for row in tbl]\n",
    "                _add_rows(rows, p, \"pdfplumber\")\n",
    "    return dedup_tables(tables)\n",
    "\n",
    "def dedup_tables(tables: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for t in tables:\n",
    "        sig = (t[\"page\"], \"\\n\".join(t[\"text\"].split(\"\\n\")[:2]))\n",
    "        if sig in seen:\n",
    "            continue\n",
    "        seen.add(sig)\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "# ===================== 4) текст и картинки =====================\n",
    "\n",
    "def extract_text_chunks(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    els = partition_pdf(\n",
    "        filename=pdf_path,\n",
    "        strategy=\"fast\",\n",
    "        chunking_strategy=\"by_title\",\n",
    "        languages=[\"ru\",\"en\"],\n",
    "        max_characters=3000,\n",
    "        new_after_n_chars=2200,\n",
    "        combine_under_n_chars=900,\n",
    "        overlap=200,\n",
    "        infer_table_structure=False,\n",
    "        extract_images_in_pdf=False,\n",
    "        include_page_breaks=False,\n",
    "    )\n",
    "    chunks = []\n",
    "    for e in els:\n",
    "        txt = (e.text or \"\").strip()\n",
    "        if not txt: \n",
    "            continue\n",
    "        chunks.append({\n",
    "            \"type\": \"text\",\n",
    "            \"subtype\": e.category,\n",
    "            \"text\": txt,\n",
    "            \"page\": getattr(e.metadata, \"page_number\", None),\n",
    "            \"source_pdf\": pdf_path,\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "def extract_figures(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    figs = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    try:\n",
    "        for i in range(len(doc)):\n",
    "            imgs = doc[i].get_images(full=True)\n",
    "            for xref, *_ in imgs:\n",
    "                figs.append({\"type\":\"figure\",\"page\":i+1,\n",
    "                             \"image_xref\":xref,\"text\":\"\",\n",
    "                             \"source_pdf\":pdf_path,\n",
    "                             \"image_b_64\": \"\"})\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return figs\n",
    "\n",
    "# ===================== 5) пайплайн ============================================\n",
    "\n",
    "def parse_pdf_fast_strict_tables(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    text_pages, image_pages = screen_pages(pdf_path, min_words=10)\n",
    "    text_chunks = extract_text_chunks(pdf_path)\n",
    "    candidate_pages = pick_table_pages(pdf_path, text_pages, thr_rulings=0.3, thr_cols=0.4)\n",
    "    tables = extract_tables_on_candidates(pdf_path, candidate_pages)\n",
    "    figures = extract_figures(pdf_path)\n",
    "    prio = {\"text\":1,\"table\":2,\"figure\":3}\n",
    "    items = text_chunks + tables + figures\n",
    "    items.sort(key=lambda x: ((x.get(\"page\") or 10**9), prio.get(x[\"type\"],9)))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sha1_file(path: str, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    p = Path(path)\n",
    "    try:\n",
    "        with p.open(\"rb\") as f:\n",
    "            while True:\n",
    "                b = f.read(chunk_size)\n",
    "                if not b:\n",
    "                    break\n",
    "                h.update(b)\n",
    "        return h.hexdigest()\n",
    "    except Exception:\n",
    "        return hashlib.sha1(str(p).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _sha1_text(s: str) -> str:\n",
    "    return hashlib.sha1((s or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def attach_ids(\n",
    "    items: List[Dict[str, Any]],\n",
    "    doc_id_mode: Literal[\"file_hash\", \"path_hash\"] = \"file_hash\",\n",
    "    chunk_id_mode: Literal[\"page_local\", \"sequential\", \"content_hash\"] = \"page_local\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Добавляет doc_id, chunk_id, original_index в каждый словарь чанка.\n",
    "    Ожидается, что у чанка уже есть поля: type, subtype, text, page, source_pdf.\n",
    "    \"\"\"\n",
    "    doc_ids = {}\n",
    "    for it in items:\n",
    "        src = it.get(\"source_pdf\") or \"unknown\"\n",
    "        if src not in doc_ids:\n",
    "            if doc_id_mode == \"file_hash\":\n",
    "                doc_ids[src] = _sha1_file(src)\n",
    "            else:\n",
    "                doc_ids[src] = hashlib.sha1(str(src).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    per_page_counters = defaultdict(lambda: defaultdict(int))  \n",
    "    per_doc_counter = defaultdict(int)                         \n",
    "    seen_chunk_ids = set()\n",
    "\n",
    "    out = []\n",
    "    for idx, ch in enumerate(items):\n",
    "        src = ch.get(\"source_pdf\") or \"unknown\"\n",
    "        page = ch.get(\"page\") if ch.get(\"page\") is not None else 0\n",
    "        doc_id = doc_ids[src]\n",
    "\n",
    "        if chunk_id_mode == \"page_local\":\n",
    "            per_page_counters[doc_id][page] += 1\n",
    "            local = per_page_counters[doc_id][page]\n",
    "            chunk_id = f\"{doc_id}:p{page}:{local}\"\n",
    "        elif chunk_id_mode == \"sequential\":\n",
    "            per_doc_counter[doc_id] += 1\n",
    "            chunk_id = f\"{doc_id}:{per_doc_counter[doc_id]}\"\n",
    "        else:  \n",
    "            text = (ch.get(\"text\") or ch.get(\"content\") or \"\")\n",
    "            text = text[:1024].replace(\"\\n\", \" \").strip()\n",
    "            page = ch.get(\"page\") if ch.get(\"page\") is not None else 0\n",
    "            chunk_key = f\"{page}|{ch.get('type')}|{text}\"\n",
    "            chunk_id  = f\"{doc_id}:{_sha1_text(chunk_key)}\"\n",
    "\n",
    "        if chunk_id in seen_chunk_ids:\n",
    "            suffix = _sha1_text(f\"{chunk_id}|{idx}\")[:6]\n",
    "            chunk_id = f\"{chunk_id}:{suffix}\"\n",
    "        seen_chunk_ids.add(chunk_id)\n",
    "\n",
    "        ch2 = dict(ch)\n",
    "        ch2[\"doc_id\"] = doc_id\n",
    "        ch2[\"chunk_id\"] = chunk_id\n",
    "        ch2[\"original_index\"] = idx\n",
    "        out.append(ch2)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Собираем все метаданные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_metadata(pdf_folder: str):\n",
    "    all_items = []\n",
    "    pdf_paths = sorted([str(p) for p in Path(pdf_folder).rglob(\"*.pdf\")])\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        items = parse_pdf_fast_strict_tables(pdf_path)\n",
    "        used_pages = []\n",
    "\n",
    "        for id, item in enumerate(items):\n",
    "            if item['type'] == 'figure':\n",
    "                if item['page'] in used_pages:\n",
    "                    items.pop(id)\n",
    "                else:\n",
    "                    used_pages.append(item['page'])\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        items_w_ids = attach_ids(items, doc_id_mode=\"file_hash\", chunk_id_mode=\"page_local\")\n",
    "        all_items.extend(items_w_ids)\n",
    "\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = create_all_metadata(pdf_folder=pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_docs = []\n",
    "uniq_chunks = []\n",
    "\n",
    "for item in items:\n",
    "    if item.get('doc_id') not in uniq_docs:\n",
    "        uniq_docs.append(item.get('doc_id'))\n",
    "    if item.get('chunk_id') not in uniq_chunks:\n",
    "        uniq_chunks.append(item.get('chunk_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_image_pages = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    doc_name = item.get('source_pdf', '')\n",
    "    if str(item.get('type', '')).lower() == 'figure':\n",
    "        page = item.get('page')\n",
    "        if page is not None:\n",
    "            unique_image_pages[doc_name].add(int(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_image_pages = {k: sorted(v) for k, v in unique_image_pages.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_page_png(pdf_path: str, page: int, dpi: int = 220, \n",
    "                    max_side: int = 1600, out_path=image_output_path) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    try:\n",
    "        pg = doc[page-1]\n",
    "        scale = dpi / 72.0\n",
    "        pix = pg.get_pixmap(matrix=fitz.Matrix(scale, scale), alpha=False)\n",
    "        \n",
    "        output_folder = out_path + pdf_path.split('/')[-1].split('.')[0]\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.mkdir(output_folder)\n",
    "        out_path_new = output_folder + '/' + f\"p_{page}.png\"\n",
    "        pix.save(out_path_new)\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "    im = Image.open(out_path_new)\n",
    "    w, h = im.size\n",
    "    m = max(w, h)\n",
    "    if m > max_side:\n",
    "        ratio = max_side / m\n",
    "        im = im.resize((int(w*ratio), int(h*ratio)), Image.LANCZOS)\n",
    "        im.save(out_path_new, optimize=True, quality=100)\n",
    "    return out_path_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=2048)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "These summaries will be embedded and used to retrieve the raw image. \\\n",
    "Give a concise summary of the image that is well optimized for retrieval in russian language strictly.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/KazTelecom/kztkp_2024_rus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  58%|█████▊    | 15/26 [10:34<07:45, 42.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Maten Petroleum/matnp_2024_rus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  62%|██████▏   | 16/26 [11:24<07:09, 42.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Oasis Logistics/oasi_af_4_2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  65%|██████▌   | 17/26 [11:52<06:11, 41.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Oasis Logistics/oasif6_2024_rus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  69%|██████▉   | 18/26 [16:21<09:38, 72.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Qazaqstan Temir Joly/tmjl_2024_rus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  73%|███████▎  | 19/26 [28:43<21:10, 181.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Qazaqstan Temir Joly/tmjl_af_1_2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  77%|███████▋  | 20/26 [28:49<14:49, 148.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Qazaqstan Temir Joly/tmjlf6_2024_cons_rus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  81%|████████  | 21/26 [29:33<10:31, 126.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Rakhat/raht_af_4_2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  85%|████████▍ | 22/26 [30:48<07:37, 114.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Rakhat/rahtp_2024_rus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  88%|████████▊ | 23/26 [35:17<07:38, 152.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Teniz Capital/tcib_af_4_2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  92%|█████████▏| 24/26 [35:52<04:03, 121.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Teniz Capital/tcibp_2024_rus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs:  96%|█████████▌| 25/26 [42:43<03:20, 200.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../instructions/Dataset/Transtelecom/tcom_af_4_2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of docs: 100%|██████████| 26/26 [43:07<00:00, 99.52s/it] \n"
     ]
    }
   ],
   "source": [
    "for doc_path, pages in tqdm.tqdm(unique_image_pages.items(), desc='Number of docs'):\n",
    "    for p in pages:\n",
    "        test_image_path = render_page_png(doc_path,\n",
    "                                          page=p)\n",
    "        encoded_image = encode_image(test_image_path)\n",
    "        summary = image_summarize(encoded_image, prompt=prompt)\n",
    "        for item in items:\n",
    "            if item['source_pdf'] == doc_path and item['type'] == 'figure' and item['page'] == p:\n",
    "                item['text'] = summary\n",
    "                item['image_b_64'] = encoded_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление контекста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = \"\"\"\n",
    "Here is the full document\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\n",
    "Please give it a strict and precise summary in a range of 2500 to 3000 words in russian language for the purposes \n",
    "of using it as a knowledge base for retrieval of the chunks. Answer only with the strict and precise summary and nothing else and keep the content relevant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context in russian language to situate this chunk within the overall document for the purposes \n",
    "of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(pdf_path: str, summary_prompt: str):\n",
    "    with pdfplumber.PDF(open(file=pdf_path, mode='rb')) as pdf:\n",
    "        pages = [page.extract_text() for page in pdf.pages]\n",
    "    doc_text = ''.join(pages)\n",
    "    doc_text = doc_text.replace('\\n','')\n",
    "\n",
    "    if len(doc_text) >= 200000:\n",
    "        doc_text = doc_text[:199000]\n",
    "\n",
    "    with client.messages.stream(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=3500,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": summary_prompt.format(doc_content=doc_text)\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    ) as stream:\n",
    "        chunks = []\n",
    "        for delta in stream.text_stream:\n",
    "            chunks.append(delta)\n",
    "        final_msg = stream.get_final_message()\n",
    "    return \"\".join(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_summary = {}\n",
    "\n",
    "pdf_paths = sorted([str(p) for p in Path(pdf_dir).rglob(\"*.pdf\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in tqdm.tqdm(pdf_paths, desc=\"Creating summary for each document\"):\n",
    "    if pdf in docs_summary.keys():\n",
    "        continue\n",
    "    docs_summary[pdf] = create_summary(pdf_path=pdf, summary_prompt=SUMMARY_PROMPT)\n",
    "    time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def situate_context(doc: str, chunk: str) -> str:\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=800,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"} \n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making context for each chunk: 100%|██████████| 4822/4822 [34:08<00:00,  2.35it/s]  \n"
     ]
    }
   ],
   "source": [
    "for item in tqdm.tqdm(items, desc=\"Making context for each chunk\"):\n",
    "    if item.get('contextualized_content'):\n",
    "        continue\n",
    "    doc_path = item.get('source_pdf')\n",
    "    doc_content = docs_summary[doc_path]\n",
    "    chunk_content = item['text']\n",
    "    if not chunk_content:\n",
    "        continue\n",
    "    else:\n",
    "        response = situate_context(doc_content, chunk_content)\n",
    "        item['contextualized_content'] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дополнительная информация об аффилированных лицах компании ТОО \"QazaqGaz Onimderi\", входящей в группу компаний АО \"Самрук-Қазына\", с подробным описанием семейных связей руководства и членов правления компании, включая их родственников и статус резидентства.\n"
     ]
    }
   ],
   "source": [
    "print(items[30]['contextualized_content'].content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vectorstore, chunks):\n",
    "    store = InMemoryStore()\n",
    "\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=\"chunk_id\"\n",
    "    )\n",
    "\n",
    "    retriever.search_type = SearchType.mmr\n",
    "\n",
    "    child_docs = []\n",
    "    kv_to_store = [] \n",
    "\n",
    "    for ch in chunks:\n",
    "        original_text = (ch.get(\"text\") or \"\").strip()\n",
    "        context_text = (ch.get(\"contextualized_content\").content[0].text if ch.get(\"contextualized_content\") else \"\").strip()\n",
    "        final_text = original_text + \"\\n\\n\" + \"Context: \" + context_text\n",
    "        if not final_text:\n",
    "            continue  \n",
    "\n",
    "        parent = Document(\n",
    "            page_content=final_text,\n",
    "            metadata={\n",
    "                \"page_num\": ch.get(\"page\"),\n",
    "                \"source\": ch.get(\"source_pdf\", \"\"),\n",
    "                \"data_type\": ch.get(\"type\", \"text\"),\n",
    "                \"raw_image\": ch.get(\"image_b_64\", \"\"),\n",
    "                \"chunk_id\": ch.get(\"chunk_id\"),\n",
    "                \"original_index\": ch.get(\"original_index\")\n",
    "            },\n",
    "        )\n",
    "        kv_to_store.append((ch.get(\"chunk_id\"), parent))\n",
    "\n",
    "        child = Document(\n",
    "            page_content=final_text,\n",
    "            metadata={\n",
    "                \"chunk_id\": ch.get(\"chunk_id\"),                \n",
    "                \"page_num\": ch.get(\"page\"),\n",
    "                \"source\": ch.get(\"source_pdf\", \"\"),\n",
    "                \"data_type\": ch.get(\"type\", \"text\"),\n",
    "            },\n",
    "        )\n",
    "        child_docs.append(child)\n",
    "\n",
    "    retriever.docstore.mset(kv_to_store)\n",
    "    retriever.vectorstore.add_documents(child_docs)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    chunk_size=32                   \n",
    ")\n",
    "\n",
    "multi_vector_img = Chroma(\n",
    "    collection_name=\"multi_vector_img_all_ver2\", \n",
    "    persist_directory='../persist_dir_2',\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    multi_vector_img,\n",
    "    items\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticsearchBM25:\n",
    "    def __init__(self, index_name: str = \"contextual_bm25_index_all_ver2\"):\n",
    "        self.es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "        self.index_name = index_name\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        index_settings = {\n",
    "            \"settings\": {\n",
    "                \"similarity\": {\"default\": {\"type\": \"BM25\"}}\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\", \"analyzer\": \"russian\"},\n",
    "                    \"contextualized_content\": {\"type\": \"text\", \"analyzer\": \"russian\"},\n",
    "                    \"doc_id\": {\"type\": \"keyword\"},\n",
    "                    \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                    \"page\": {\"type\": \"integer\"},\n",
    "                    \"source_pdf\": {\"type\": \"keyword\", \"index\": False},\n",
    "                    \"data_type\": {\"type\": \"keyword\", \"index\": False},\n",
    "                    \"raw_image\": {\"type\": \"binary\"},\n",
    "                    \"original_index\": {\"type\": \"integer\"} \n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if not self.es_client.indices.exists(index=self.index_name):\n",
    "            self.es_client.indices.create(index=self.index_name, body=index_settings)\n",
    "            print(f\"Created index: {self.index_name}\")\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict[str, Any]]):\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"content\": doc[\"text\"],\n",
    "                    \"contextualized_content\": doc.get(\"contextualized_content\").content[0].text if doc.get(\"contextualized_content\") else \"\",\n",
    "                    \"doc_id\": doc[\"doc_id\"],\n",
    "                    \"chunk_id\": doc[\"chunk_id\"],\n",
    "                    \"page\": doc[\"page\"],\n",
    "                    \"source_pdf\": doc[\"source_pdf\"],\n",
    "                    \"data_type\": doc[\"type\"],\n",
    "                    \"raw_image\": doc.get(\"raw_image\"),\n",
    "                    \"original_index\": doc[\"original_index\"]\n",
    "                },\n",
    "            }\n",
    "            for doc in documents\n",
    "        ]\n",
    "        success, _ = bulk(self.es_client, actions)\n",
    "        self.es_client.indices.refresh(index=self.index_name)\n",
    "        return success\n",
    "    \n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        self.es_client.indices.refresh(index=self.index_name) \n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"content\", \"contextualized_content\"],\n",
    "                }\n",
    "            },\n",
    "            \"size\": k,\n",
    "        }\n",
    "        response = self.es_client.search(index=self.index_name, body=search_body)\n",
    "        return [\n",
    "            {\n",
    "                \"content\": hit[\"_source\"][\"content\"],\n",
    "                \"contextualized_content\": hit[\"_source\"][\"contextualized_content\"],\n",
    "                \"doc_id\": hit[\"_source\"][\"doc_id\"],\n",
    "                \"chunk_id\": hit[\"_source\"][\"chunk_id\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "                \"page\": hit[\"_source\"][\"page\"],\n",
    "                \"source_pdf\": hit[\"_source\"][\"source_pdf\"],\n",
    "                \"data_type\": hit[\"_source\"][\"data_type\"],\n",
    "                \"raw_image\": hit[\"_source\"][\"raw_image\"],\n",
    "                \"original_index\": hit[\"_source\"][\"original_index\"]\n",
    "            }\n",
    "            for hit in response[\"hits\"][\"hits\"]\n",
    "        ]\n",
    "\n",
    "def create_elasticsearch_bm25_index(data):\n",
    "    es_bm25 = ElasticsearchBM25()\n",
    "    es_bm25.index_documents(data)\n",
    "    return es_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/_sr2y9h11m9950k06c5s5vb00000gn/T/ipykernel_59020/475135864.py:27: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  self.es_client.indices.create(index=self.index_name, body=index_settings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: contextual_bm25_index_all_ver2\n"
     ]
    }
   ],
   "source": [
    "es_bm25 = create_elasticsearch_bm25_index(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_advanced(query: str, db: MultiVectorRetriever, es_bm25: ElasticsearchBM25, \n",
    "                      k: int, \n",
    "                      docs: List[Dict[str, Any]],\n",
    "                      semantic_weight: float = 0.7, bm25_weight: float = 0.3,\n",
    "                      num_chunks_to_recall: int = 50):\n",
    "\n",
    "    semantic_results = db.invoke(query, k=num_chunks_to_recall)\n",
    "    ranked_chunk_ids = [(result.metadata['chunk_id'], result.metadata['original_index']) for result in semantic_results]\n",
    "\n",
    "    bm25_results = es_bm25.search(query, k=num_chunks_to_recall)\n",
    "    ranked_bm25_chunk_ids = [(result['chunk_id'], result['original_index']) for result in bm25_results]\n",
    "\n",
    "    chunk_ids = list(set(ranked_chunk_ids + ranked_bm25_chunk_ids))\n",
    "    chunk_id_to_score = {}\n",
    "\n",
    "    for chunk_id in chunk_ids:\n",
    "        score = 0\n",
    "        if chunk_id in ranked_chunk_ids:\n",
    "            index = ranked_chunk_ids.index(chunk_id)\n",
    "            score += semantic_weight * (1 / (index + 1))  \n",
    "        if chunk_id in ranked_bm25_chunk_ids:\n",
    "            index = ranked_bm25_chunk_ids.index(chunk_id)\n",
    "            score += bm25_weight * (1 / (index + 1)) \n",
    "        chunk_id_to_score[chunk_id] = score\n",
    "\n",
    "    sorted_chunk_ids = sorted(\n",
    "        chunk_id_to_score.keys(), key=lambda x: (chunk_id_to_score[x], x[0], x[1]), reverse=True\n",
    "    )\n",
    "\n",
    "    # -------------------- Cohere Rerank (минимальное добавление) --------------------\n",
    "    api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "    if api_key and sorted_chunk_ids:\n",
    "        co = cohere.Client(api_key)\n",
    "\n",
    "        doc_map = { (d[\"chunk_id\"], d[\"original_index\"]) : d for d in docs }\n",
    "\n",
    "        top_n_rr = min(len(sorted_chunk_ids), max(5 * k, 50))\n",
    "        candidates = sorted_chunk_ids[:top_n_rr]\n",
    "\n",
    "        docs_for_rr = []\n",
    "        kept_idx = []\n",
    "        for idx, cid in enumerate(candidates):\n",
    "            d = doc_map.get(cid)\n",
    "            if not d:\n",
    "                continue\n",
    "            base = d.get(\"text\")\n",
    "            ctx  = d.get(\"contextualized_content\")\n",
    "            if isinstance(ctx, str):\n",
    "                ctx_text = ctx\n",
    "            else:\n",
    "                try:\n",
    "                    ctx_text = ctx.content[0].text \n",
    "                except Exception:\n",
    "                    ctx_text = \"\"\n",
    "            txt = (base + (\"\\n\\n\" + ctx_text if ctx_text else \"\")).strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            docs_for_rr.append({\"text\": txt})\n",
    "            kept_idx.append(idx)\n",
    "\n",
    "        if docs_for_rr:\n",
    "            rr = co.rerank(\n",
    "                model=\"rerank-v3.5\",\n",
    "                query=query,\n",
    "                documents=docs_for_rr,\n",
    "                top_n=len(docs_for_rr),\n",
    "            )\n",
    "            order = sorted(rr.results, key=lambda r: r.relevance_score, reverse=True)\n",
    "            reranked_ids = [candidates[kept_idx[r.index]] for r in order]\n",
    "            remainder = [cid for cid in candidates if cid not in set(reranked_ids)]\n",
    "            tail = [cid for cid in sorted_chunk_ids if cid not in set(candidates)]\n",
    "            sorted_chunk_ids = reranked_ids + remainder + tail\n",
    "\n",
    "    for index, chunk_id in enumerate(sorted_chunk_ids):\n",
    "        chunk_id_to_score[chunk_id] = 1 / (index + 1)\n",
    "\n",
    "    final_results = []\n",
    "    semantic_count = 0\n",
    "    bm25_count = 0\n",
    "    for chunk_id in sorted_chunk_ids[:k]:\n",
    "        chunk_metadata = next(chunk for chunk in docs if chunk['chunk_id'] == chunk_id[0] and chunk['original_index'] == chunk_id[1])\n",
    "        is_from_semantic = chunk_id in ranked_chunk_ids\n",
    "        is_from_bm25 = chunk_id in ranked_bm25_chunk_ids\n",
    "        final_results.append({\n",
    "            'chunk': chunk_metadata,\n",
    "            'score': chunk_id_to_score[chunk_id],\n",
    "            'from_semantic': is_from_semantic,\n",
    "            'from_bm25': is_from_bm25\n",
    "        })\n",
    "        \n",
    "        if is_from_semantic and not is_from_bm25:\n",
    "            semantic_count += 1\n",
    "        elif is_from_bm25 and not is_from_semantic:\n",
    "            bm25_count += 1\n",
    "        else: \n",
    "            semantic_count += 0.5\n",
    "            bm25_count += 0.5\n",
    "\n",
    "    return final_results, semantic_count, bm25_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_page_content(ch: dict) -> str:\n",
    "    text = (ch.get(\"text\") or ch.get(\"content\") or \"\")  \n",
    "    ctx  = (ch.get(\"contextualized_content\").content[0].text if ch.get(\"contextualized_content\") else \"\") \n",
    "    return (text + (\"  \" + ctx if ctx else \"\")).strip()\n",
    "\n",
    "def make_hybrid_runnable(\n",
    "    db, es_bm25, docs,\n",
    "    top_k_out: int = 20,\n",
    "    recall: int = 50,\n",
    "    semantic_weight: float = 0.7,\n",
    "    bm25_weight: float = 0.3\n",
    "):\n",
    "    \"\"\"Возвращает Runnable, который принимает строку-вопрос и отдаёт List[Document].\"\"\"\n",
    "    def _search(query: str):\n",
    "        results, _, _ = retrieve_advanced(\n",
    "            query=query,\n",
    "            db=db,\n",
    "            es_bm25=es_bm25,\n",
    "            k=top_k_out,                   \n",
    "            docs=docs,                         \n",
    "            semantic_weight=semantic_weight,\n",
    "            bm25_weight=bm25_weight,\n",
    "            num_chunks_to_recall=recall\n",
    "        )\n",
    "        out = []\n",
    "        for r in results:\n",
    "            ch = r[\"chunk\"]\n",
    "            out.append(\n",
    "                Document(\n",
    "                    page_content=_build_page_content(ch),\n",
    "                    metadata={\n",
    "                        \"chunk_id\": ch[\"chunk_id\"],\n",
    "                        \"doc_id\": ch[\"doc_id\"],\n",
    "                        \"original_index\": ch[\"original_index\"],\n",
    "                        \"page_num\": ch.get(\"page\"),\n",
    "                        \"source\": ch.get(\"source_pdf\"),\n",
    "                        \"data_type\": ch.get(\"type\"),\n",
    "                        \"raw_image\": ch.get(\"raw_image\"), \n",
    "                        \"score\": r[\"score\"],\n",
    "                        \"from_semantic\": r[\"from_semantic\"],\n",
    "                        \"from_bm25\": r[\"from_bm25\"]\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        return out\n",
    "    return RunnableLambda(_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = make_hybrid_runnable(\n",
    "    db=retriever_multi_vector_img,\n",
    "    es_bm25=es_bm25,     \n",
    "    docs=items\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-modal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages_with_indices(payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    payload: {\"docs\": List[Document], \"question\": str, \"question_id\": int, \"answer_type\": str}\n",
    "    Возвращает {\"messages\": [HumanMessage], \"docs\": docs, \"question_id\": ...}\n",
    "    \"\"\"\n",
    "    docs: List[Document] = payload[\"docs\"]\n",
    "    question: str = payload.get(\"question\") or payload.get(\"full_question\") or \"\"\n",
    "    answer_type: str = payload[\"answer_type\"]\n",
    "\n",
    "    instruction = (\n",
    "        \"Отвечай на русском языке ТОЛЬКО на основе фрагментов ниже.\\n\"\n",
    "        \"Каждый фрагмент помечен индексом в квадратных скобках, например [0].\\n\"\n",
    "        \"Если в вопросе просят указать должность, то напиши его должность целиком и обязательно проверь, что имена в вопросе и найденных фрагментах совпадают.\\n\"\n",
    "        \"Часто ответ на такой вопрос имеется в таблицах, проверяй, что ты берешь информацию из нужного ряда.\\n\"\n",
    "        \"Верни JSON ровно такого вида и ничего больше:\\n\"\n",
    "        '{ \"answer\": \"<строка ответа>\", \"evidence\": [<индексы фрагментов, по убыванию важности>] }\\n'\n",
    "        \"В поле answer верни только ответ на вопрос, без дополнительных пояснений и знаков препинания в конце.\\n\"\n",
    "        f\"Ожидаемый тип данных ответа на вопрос - {answer_type}.\\n\"\n",
    "        \"Если ожидаемый тип данных int либо float либо в вопросе просят указать количество чего-то, либо назвать какое-то число (баллов, очков и т.д.), то в ответе напиши ТОЛЬКО цифру, без пояснений, меры измерения и валюты.\\n\"\n",
    "        \"Не отделяй разряды пробелом, например 123 000 должно быть 123000.\\n\"\n",
    "        \"При этом, если ожидаемый тип данных float, то в ответе обязательно должна быть дробная часть, отдели его точкой, например 6.2 или 14.75.\\n\"\n",
    "        \"Если использован один фрагмент — верни один индекс.\\n\"\n",
    "        \"Если для ответа на вопрос использовано несколько фрагментов, например необходимо сравнить информацию на разных страницах либо в разных документах - верни несколько индексов.\\n\"\n",
    "        \"Не добавляй лишний текст вне JSON.\"\n",
    "    )\n",
    "\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": instruction},\n",
    "        {\"type\": \"text\", \"text\": f\"Вопрос: {question}\"},\n",
    "    ]\n",
    "\n",
    "    for i, d in enumerate(docs):\n",
    "        src = d.metadata.get(\"source\") or d.metadata.get(\"source_pdf\") or \"\"\n",
    "        name = Path(src).name if src else \"\"\n",
    "        page = d.metadata.get(\"page_num\") or d.metadata.get(\"page\")\n",
    "\n",
    "        if d.metadata.get(\"data_type\") == \"figure\" and d.metadata.get(\"raw_image\"):\n",
    "            content.append({\"type\": \"text\", \"text\": f\"[{i}] IMAGE — {name}, стр. {page}.\"})\n",
    "            img = d.metadata[\"raw_image\"]\n",
    "            url = img if str(img).startswith(\"data:\") else f\"data:image/png;base64,{img}\"\n",
    "            content.append({\"type\": \"image_url\", \"image_url\": {\"url\": url}})\n",
    "        else:\n",
    "            txt = d.page_content or \"\"\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"[{i}] TEXT — {name}, стр. {page}\\n{txt}\"\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=content)],\n",
    "        \"docs\": docs,\n",
    "        \"question_id\": payload[\"question_id\"],\n",
    "        \"answer_type\": payload[\"answer_type\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_json(s: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        start = s.find(\"{\")\n",
    "        end = s.rfind(\"}\")\n",
    "        if start == -1 or end == -1:\n",
    "            return {\"answer\": s.strip(), \"evidence\": []}\n",
    "        data = json.loads(s[start:end+1])\n",
    "        # подстрахуем поля\n",
    "        if not isinstance(data, dict):\n",
    "            return {\"answer\": s.strip(), \"evidence\": []}\n",
    "        ans = str(data.get(\"answer\", \"\")).strip()\n",
    "        ev = data.get(\"evidence\", [])\n",
    "        if isinstance(ev, int):\n",
    "            ev = [ev]\n",
    "        if not isinstance(ev, list):\n",
    "            ev = []\n",
    "        ev = [int(x) for x in ev if isinstance(x, (int, float))] \n",
    "        return {\"answer\": ans or s.strip(), \"evidence\": ev}\n",
    "    except Exception:\n",
    "        return {\"answer\": s.strip(), \"evidence\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_payload(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    inputs: {\n",
    "      \"answer_text\": \"<raw model output>\",\n",
    "      \"docs\": List[Document],\n",
    "      \"question_id\": int,\n",
    "      \"answer_type\": str\n",
    "    }\n",
    "    \"\"\"\n",
    "    parsed = parse_model_json(inputs[\"answer_text\"])\n",
    "    answer_text: str = parsed[\"answer\"]\n",
    "    if inputs[\"answer_type\"] == 'float':\n",
    "        try:\n",
    "          answer_text = float(answer_text)\n",
    "        except ValueError:\n",
    "           pass\n",
    "    elif inputs[\"answer_type\"] == 'int':\n",
    "        try:\n",
    "          answer_text = int(answer_text)   \n",
    "        except ValueError:\n",
    "           try:\n",
    "              answer_text = float(answer_text)\n",
    "           except ValueError:\n",
    "              pass\n",
    "    ev: List[int] = parsed[\"evidence\"] or []\n",
    "\n",
    "    docs: List[Document] = inputs[\"docs\"]\n",
    "    qid: int = int(inputs[\"question_id\"])\n",
    "\n",
    "    idx = ev[0] if ev and 0 <= ev[0] < len(docs) else 0\n",
    "    d = docs[idx]\n",
    "    src = d.metadata.get(\"source\") or d.metadata.get(\"source_pdf\") or \"\"\n",
    "    name = Path(src).name if src else \"\"\n",
    "    page = d.metadata.get(\"page_num\") or d.metadata.get(\"page\")\n",
    "\n",
    "    return {\n",
    "        \"question_id\": qid,\n",
    "        \"relevant_chunks\": [{\"document_name\": name, \"page_number\": int(page) if page is not None else None}],\n",
    "        \"answer\": answer_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_structured_chain_that_cites_one(hybrid_retriever_runnable, llm=None):\n",
    "    model = llm or ChatOpenAI(temperature=0, model=\"gpt-5\", max_tokens=2048)\n",
    "\n",
    "    chain = (\n",
    "    {\n",
    "        \"docs\": (itemgetter(\"full_question\") | hybrid_retriever),\n",
    "        \"question\": itemgetter(\"full_question\"),\n",
    "        \"question_id\": itemgetter(\"id\"),\n",
    "        \"answer_type\": itemgetter(\"answer_type\")\n",
    "    }\n",
    "    | RunnableLambda(build_messages_with_indices)\n",
    "    | {\n",
    "        \"answer_text\": (itemgetter(\"messages\") | model | StrOutputParser()),\n",
    "        \"docs\": itemgetter(\"docs\"),\n",
    "        \"question_id\": itemgetter(\"question_id\"),\n",
    "        \"answer_type\": itemgetter(\"answer_type\")\n",
    "    }\n",
    "    | RunnableLambda(build_final_payload)\n",
    ")\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_structured = build_structured_chain_that_cites_one(hybrid_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_excel('../instructions/questions_private.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>block</th>\n",
       "      <th>full_question</th>\n",
       "      <th>answer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>OCR</td>\n",
       "      <td>Сколько пассажирских вагонов у АО «НК «ҚТЖ» (в...</td>\n",
       "      <td>float</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>OCR</td>\n",
       "      <td>Какой объем добычи нефти (тыс. тонн) с месторо...</td>\n",
       "      <td>float</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>AR</td>\n",
       "      <td>Сколько шоколадных цехов есть у АО “Баян Сулу”?</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>Both</td>\n",
       "      <td>Какую долю (в процентах) в грузообороте АО «НК...</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>Both</td>\n",
       "      <td>Какова суммарная доля участия членов Совета Ди...</td>\n",
       "      <td>float</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id block                                      full_question answer_type\n",
       "195  196   OCR  Сколько пассажирских вагонов у АО «НК «ҚТЖ» (в...       float\n",
       "196  197   OCR  Какой объем добычи нефти (тыс. тонн) с месторо...       float\n",
       "197  198    AR    Сколько шоколадных цехов есть у АО “Баян Сулу”?         int\n",
       "198  199  Both  Какую долю (в процентах) в грузообороте АО «НК...         int\n",
       "199  200  Both  Какова суммарная доля участия членов Совета Ди...       float"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "\n",
    "for id, row in tqdm.tqdm(questions.iterrows(), desc=\"Answering questions...\"):\n",
    "    question_payload = {}\n",
    "    question_payload['id'] = row['id']\n",
    "    question_payload['full_question'] = row['full_question']\n",
    "    question_payload['answer_type'] = row['answer_type']\n",
    "    result = chain_structured.invoke(question_payload)\n",
    "    print(f\"Answer: {result['answer']} --- Question_ID: {result['question_id']} --- DOC: {result['relevant_chunks'][0]['document_name']} --- Page: {result['relevant_chunks'][0]['page_number']}\")\n",
    "    ans.append(result)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"our_answers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ans, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
